

* Apache Kafka

Kafka: Fast delegate, evolução e cluster de brokers

Use servidor http como ponto de entrada
Evolua os serviços e schemas
Faça seu serviço acessar bancos externos
Conheça como fazer a replicação de clusters

--------------------------------------------------------------------------------

1. Ligar o zookeeper
$ /Documents/Desenvolvimento/eclipse-workspace/JmsKafka/kafka_2.13-2.6.0
$ ./bin/zookeeper-server-start.sh ./config/zookeeper.properties

2. Ligar o kafka
carloss@carloss-note:~/Documents/Desenvolvimento/eclipse-workspace/JmsKafka/kafka_2.13-2.6.0
$ ./bin/kafka-server-start.sh ./config/server.properties

--------------------------------------------------------------------------------
*** Aula 01 - Novos produtores e consumidores;

Como fazer um consumidor também produzir
Como lidar com patterns e novos topics
Como acessar um banco de dados
Problemas de schema que vão sendo levantados durante a evolução dos serviços

* crio um modulo service-users

	Add as dependencias do modulo sqlite-jdbc no pom.xml do service-users
	mvnrepository sqlite
	https://mvnrepository.com/artifact/org.xerial/sqlite-jdbc
	https://mvnrepository.com/artifact/org.xerial/sqlite-jdbc/3.32.3.2
	no service-users/pom.xml eu coloco o sqlite;

	Add as dependencias do modulo common-kafka no pom.xml do service-users





--------------------------------------------------------------------------------
*** Aula 02 - Evoluindo serviços e schemas;

como evoluir um serviço sem quebrar os schemas
como pensar a evolução de um serviço
discutindo UUID e id único


--------------------------------------------------------------------------------
*** Aula 03 - Usando um servidor http como ponto de entrada


como usar um servidor http embarcado
como criar um serviço http
como enviar mensagens a partir do servidor http
a vantagem de um fast delegate


* criar modulo service-http-ecommerce

	add dependencia no pom:

	mvnrepository jetty
	https://mvnrepository.com/artifact/org.eclipse.jetty/jetty-servlet
	https://mvnrepository.com/artifact/org.eclipse.jetty/jetty-servlet/9.4.32.v20200930

	Add as dependencias do modulo common-kafka no pom.xml do service-users

	Codificar conforme o projeto e testar
	http://localhost:8080/new

	http://localhost:8080/new?email=carlosofpersia@hotmail.com&amount=157
	http://localhost:8080/new?email=carlosofpersia@hotmail.com&amount=5157




--------------------------------------------------------------------------------
*** Aula 04 - Clusters de brokers

* Se a particao cai, eu posso ter varias e outro servico pega, mas se o broker (zookeper ou kafka)  cai, eu so tenho um.

	o problema do single point of failure
	a recuperação mais simples de um broker
	a recuperação e o rebalanceamento de um serviço
	como levantar mais um broker e rodar um cluster
	como efetuar a replicação
	o que é um líder
	a configuração do acks 0, 1 e all
	como utilizar garantias, reliability, velocidade, partições e replicação

* listar os topicos para ver o numero de particoes
./bin/kafka-topics.sh --describe --bootstrap-server localhost:9092

* aumentar numero de particoes:
./bin/kafka-topics.sh --alter --zookeeper localhost:2181 --topic ECOMMERCE_NEW_ORDER --partitions 3

-----------------------------------


********************************************************************
-----------------------------------
* 01 - Replicação em cluster
	Configurar um segundo servidor:
		cp config/server.properties config/server2.properties
		vi config/server2.properties

			#
			broker.id = 2
			#
			log.dirs=/home/carloss/Documents/Desenvolvimento/eclipse-workspace/JmsKafka/data/kafka2
			#
			listeners=PLAINTEXT://:9093

		$ ./bin/kafka-server-start.sh ./config/server2.properties

	* Rodo a lista de topicos com suas particoes para ver o que esta acontecendo (Leader: 0	Replicas: 0): 

	$ ./bin/kafka-topics.sh --describe --bootstrap-server localhost:9092

		Topic: ECOMMERCE_NEW_ORDER	PartitionCount: 3	ReplicationFactor: 1	Configs: segment.bytes=1073741824
			Topic: ECOMMERCE_NEW_ORDER	Partition: 0	Leader: 0	Replicas: 0	Isr: 0
			Topic: ECOMMERCE_NEW_ORDER	Partition: 1	Leader: 0	Replicas: 0	Isr: 0
			Topic: ECOMMERCE_NEW_ORDER	Partition: 2	Leader: 0	Replicas: 0	Isr: 0

	* Apontar as particoes para as portas do kafka.properties...

		Add um fator de replicacao do kafka para pegar mais brokers (replication-factor):

	$ vi config/server.properties
		#
		broker.id=0...
		default.replication.factor = 2

	$ vi config/server2.properties
		#
		broker.id=2...
		default.replication.factor = 2

		fecho tudo e limpo os dados de data.. 
		cd ../Documents/Desenvolvimento/eclipse-workspace/JmsKafka/kafka_2.13-2.6.0
			$ rm -rf ../data/kafka/*
			$ rm -rf ../data/kafka2/*
			$ rm -rf ../data/zookeeper/*

		e roda novamente o zookeper o kafka server 1 e 2...

			$ ./bin/zookeeper-server-start.sh ./config/zookeeper.properties
			$ ./bin/kafka-server-start.sh ./config/server.properties
			$ ./bin/kafka-server-start.sh ./config/server2.properties

		* ligo tudo, e vejo o que ta pegando:
			$ ./bin/kafka-topics.sh --describe --bootstrap-server localhost:9092

	Topic: ECOMMERCE_NEW_ORDER	PartitionCount: 3	ReplicationFactor: 2 ->
		Topic: ECOMMERCE_NEW_ORDER	Partition: 0	Leader: 2	Replicas: 2,0	Isr: 2,0
		Topic: ECOMMERCE_NEW_ORDER	Partition: 1	Leader: 0	Replicas: 0,2	Isr: 0,2
		Topic: ECOMMERCE_NEW_ORDER	Partition: 2	Leader: 2	Replicas: 2,0	Isr: 2,0

	* obs:
		"Isr: 0, 2" -> sao as que estao atualizadas e ativas.
		- Um derrubou o outro que esta de pe vira lider. 

********************************************************************
-----------------------------------
* 02 - Cluster de 5 brokers e explorando líderes e réplicas
/Documents/Desenvolvimento/eclipse-workspace/JmsKafka/kafka_2.13-2.6.0
	
Deve-se replicar o __consumer_offset -> recomendado para 3.

$ vi config/server.properties

default.replication.factor=3

offsets.topic.replication.factor=3
transaction.state.log.replication.factor=3

* Vamos criar 4 server.properties. -> 4 brokers kafka.

$ cd config;
$ mv server.properties server1.properties;
$ cp server1.properties server2.properties;
$ cp server1.properties server3.properties;
$ cp server1.properties server4.properties;
$ cp server1.properties server4.properties;

Configirar para seus respectivos numeros:

$ vi config/server1.properties

	broker.id=1
	default.replication.factor=3
	listeners=PLAINTEXT://:9091
	log.dirs=/home/carloss/Documents/Desenvolvimento/eclipse-workspace/JmsKafka/data/kafka1


$ vi config/server2.properties

	broker.id=2
	default.replication.factor=3
	listeners=PLAINTEXT://:9092
	log.dirs=/home/carloss/Documents/Desenvolvimento/eclipse-workspace/JmsKafka/data/kafka2
	
$ vi config/server3.properties

	broker.id=3
	default.replication.factor=3
	listeners=PLAINTEXT://:9093
	log.dirs=/home/carloss/Documents/Desenvolvimento/eclipse-workspace/JmsKafka/data/kafka3
	
$ vi config/server4.properties

	broker.id=4
	default.replication.factor=3
	listeners=PLAINTEXT://:9094
	log.dirs=/home/carloss/Documents/Desenvolvimento/eclipse-workspace/JmsKafka/data/kafka4


$ vi config/server5.properties

	broker.id=5
	default.replication.factor=3
	listeners=PLAINTEXT://:9095
	log.dirs=/home/carloss/Documents/Desenvolvimento/eclipse-workspace/JmsKafka/data/kafka5


* Apago os datas para poder zerar 
$ rm -rf ../data/*

--//--

* Rodar

./bin/zookeeper-server-start.sh ./config/zookeeper.properties

./bin/kafka-server-start.sh ./config/server1.properties
./bin/kafka-server-start.sh ./config/server2.properties
./bin/kafka-server-start.sh ./config/server3.properties
./bin/kafka-server-start.sh ./config/server4.properties
./bin/kafka-server-start.sh ./config/server5.properties

--//--

Testar com as portas, caso a outra caia.
./bin/kafka-topics.sh --describe --bootstrap-server localhost:9091
./bin/kafka-topics.sh --describe --bootstrap-server localhost:9092
./bin/kafka-topics.sh --describe --bootstrap-server localhost:9093
./bin/kafka-topics.sh --describe --bootstrap-server localhost:9094

// usar com zookeeper.
./bin/kafka-topics.sh --describe --zookeeper localhost:2181



********************************************************************
-----------------------------------
* 03 - Acks e reliability (ACKS_CONFIG)

	em KafkaDispatcher.java no metodo properties() ...

        // somente da ok, quando todas as replicas confirmarem, isso e bom para garantir que se um cair esta no outro. (0, 1, all)
        0 - se você for certo pode ser um arquivo de configuração ou a gente conversão, então o produtor não vai esperar, manda mensagem e nem espera o líder dizer que está ok (que se lasque);
        1 - o líder vai escrever no log local
        all - todas replicas em sinc devem confirmar;

        #
        properties.setProperty(ProducerConfig.ACKS_CONFIG, "all");



./bin/kafka-consumer-groups.sh --all-groups --bootstrap-server localhost:9092 --describe

-----------------



* Teoria sobre garantia das informacoes de acordo com ACKS_CONFIG

	Porque nós que enviamos uma mensagem, HttpEcommerceService, quando enviamos a nossa mensagem, no send, chamamos um get e ele retornou quando o líder ficou sabendo que estava tudo ok.
	O mais seguro é esperar o líder mandar para as réplicas e elas confirmarem, é lento. Porém você tem mais garantia, um pouco de troca da velocidade com as garantias que você quer ter. Eu quero ter garantia que tudo é executado exatamente numa ordem, então tem de ser serializado.

	Um depois do outro. Eu quero ter garantia de que se um cai, as informações estão em outro lugar, então quando você envia a mensagem você só vai ter o ok, quando a mensagem for replicada nas réplicas. Configura isso no Dispatcher, tem uma propriedade, set propertie, producerConfig.ACK, os oks do servidor, quantos eu quero ter? O número de acknowledgments que o producer quer do líder para ter certeza de que o request foi completado.

	* Eu quero quantos? Aqui você tem os valores que você pode colocar acks = 0 se você for certo pode ser um arquivo de configuração ou a gente conversão, então o produtor não vai esperar, manda mensagem e nem espera o líder dizer que está ok. Há mensagens que se você realmente não vai lidar, não vai se preocupar.

	Não estou nem se eu devo escrever ou não só quero mandar mensagem e perder uma outra azar. Pode ser, pode ter situações assim, você vai por zero e é mais rápido, você sai processando, ele vai automaticamente adicionar no socket e acabou; vai ser considerado que foi enviado, nem necessariamente foi enviado.

	Não existe garantia de que o servidor recebeu. E a configuração de retries que o servidor fica retentando se não está lá vai ser ignorada, porque se ele não recebeu, tudo bem, é isso que você está dizendo com acks zero.

	* Em geral você não fica sabendo das falhas. Existem outras configurações, acks=1, quer dizer que o líder vai escrever no log local, o líder não vai esperar que as réplicas tenham recebido a mensagem e confirmado. Um significa isso.

	* Se o líder, recebeu, gravou e falhou, as réplicas não ficaram sabendo, será perdida essa informação. E acks igual a all significa que o líder vai esperar todas as réplicas que estão sync, todas elas estão sincronizadas, todas elas receberam essa informação, agora eu posso confirmar que a sua mensagem foi enviada.

	Se líder cair as réplicas têm essa informação. Quero usar acks config all, eu vou esperar todas as réplicas terem essa informação. Uma configuração super simples provavelmente o seu use case padrão vai querer isso, porque você provavelmente quer garantia de que se um cair está com o outro a mensagem está na réplica.

	Provavelmente se você quer isso o padrão será este. Você pode deixar o seu programador sobrescrever essa propriedade da maneira que quiser. Você quer um valor padrão para toda a empresa, para todo seu projeto. No nosso caso eu quero garantir que a mensagem esteja em pelo menos mais dois lugares, três lugares.

	Porque os meus tópicos têm por padrão replication factor três, dessa maneira garante, só isso porque que agora o send.get vai esperar o acks do líder - falar as réplicas já foram sincronizadas, na verdade pode parar, reestartar o nosso HttpEcommerceService, agora ele já está usando essa configuração nova e vai esperar as outras réplicas.

