

Apache Kafka 3

* Dead Letter Queue e Garantias de entrega

Generalize um processo de batch assíncrono
Entenda melhor a importância de fast delegate e pontas do sistemas
Entenda a importância e implemente um correlation ID
Implemente uma versão inicial de deadletter
Entenda os problemas de retries assíncronos

Agora que você já tem bom conhecimento sobre Kafka e a arquitetura de microserviços avance no uso dela. Replica os seus dados e saiba como lidar com erros no processamento. Aprenda como definir um dead letter queue e tentativas de reenvio. Crie múltiplos tópicos para o mesmo serviço e saiba como obter garantias sobre o envio e entrega de mensagens.


--------------------------------------------------------------------------------

1. Ligar o zookeeper
$ cd /home/carloss/Documents/Desenvolvimento/eclipse-workspace/JmsKafka/kafka_2.13-2.6.0
$ ./bin/zookeeper-server-start.sh ./config/zookeeper.properties

2. Ligar o kafka
$ cd /home/carloss/Documents/Desenvolvimento/eclipse-workspace/JmsKafka/kafka_2.13-2.6.0
$ ./bin/kafka-server-start.sh ./config/server.properties

--------------------------------------------------------------------------------

colocar em todos os modulos de servicos do InteliJ: Run -> Edit Configurations -> Workin directory:
$MODULE_WORKING_DIR$


**************************************************
-------------------------------------------------
01.2 - Simulando a geração de relatórios

criando um novo serviço que faz IO
consideramos o acesso a disco como serviço externo
diversas formas de trabalhar batch
usando o batch com http fast delegate
usando um processo assíncrono e mantendo o isolamento do banco de usuários

	* service-reading-report
		ReadingReportService

	* service-http-ecommerce
		GenerateAllReportsServlet
		HttpEcommerceService

	* service-users			
		BatchSendMessageService (BATCH)



**************************************************
-------------------------------------------------
02.2 - A importância de um CorrelationId

	A chave sera uma string que sera concatenada ...


02.3 - A serialização customizada com correlation id e um wrapper

	commom-kafka
		CorrelationId
		Message
		KafkaDispatcher

	rm -rf ../data/*

		MessageAdapter
		GsonSerializer


02.4 - Deserialização customizada

		GsonDeserializer
		BatchSendMessageService
		ReadingReportService
		CreateUserService


http://localhost:8080/admin/generate-reports


* O que aprendemos?

a importância de um correlation id
serialização e deserialização customizada em sua própria camada
wrapping de mensagens com tipo próprio



**************************************************
-------------------------------------------------
03.3 - Implementando o correlation id

* Quem quer usar o Kafka tem que usar o CorrelationId

como implementar um correlation id
a importância da mensagem como wrapper ou headers
como manter o histórico de mensagens que geraram uma determinada mensagem


	common-kafka
		CorrelationId
		KafkaDispatcher

	service-http-ecommerce
		HttpEcommerceService
		GenerateAllReportsServlet
		NewOrderServlet

	service-users
		BatchSendMessageService
		CreateUserService

	service-log
		LogService

	service-frauddetector
		FraudDetectorService

	service-new-order
		NewOrderMain

	service-reading-report	
		ReadingReportService

	service-email
		EmailService



**************************************************
-------------------------------------------------
03.4 - Revisando a arquitetura até agora


revisando tópicos e partições
revisando consumer groups
revisando líderes e réplicas
revisando rebalanceamento

	todos os servicos rodando, vamos ver alguns detalhes

	* add qtd de particoes.
	./bin/kafka-topics.sh --alter --zookeeper localhost:2181 --topic ECOMMERCE_ORDER_APPROVED --partitions 3
	./bin/kafka-topics.sh --alter --zookeeper localhost:2181 --topic ECOMMERCE_ORDER_REJECTED --partitions 3
	./bin/kafka-topics.sh --alter --zookeeper localhost:2181 --topic ECOMMERCE_USER_GENERATE_READING_REPORT --partitions 3
	./bin/kafka-topics.sh --alter --zookeeper localhost:2181 --topic ECOMMERCE_SEND_EMAIL --partitions 3
	./bin/kafka-topics.sh --alter --zookeeper localhost:2181 --topic ECOMMERCE_SEND_MESSAGE_TO_ALL_USERS --partitions 3
	./bin/kafka-topics.sh --alter --zookeeper localhost:2181 --topic ECOMMERCE_NEW_ORDER --partitions 3

	* listar os topicos para ver o numero de particoes
	./bin/kafka-topics.sh --describe --bootstrap-server localhost:9092

	* ver todos os topicos por grupos 
	./bin/kafka-consumer-groups.sh --all-groups --bootstrap-server localhost:9092 --describe



-------------------------------------------------
03.4 - Colocando property para saber de quantas em quantas mensagens eu quero consumir por particao.




* As particoes sao rebalanceadas.

o poll vai pingando no servidor avisando que esta vivo.

	var records = consumer.poll(Duration.ofMillis(100));

quando subo outra particao, o zookeeper faz um rebalanco.

quando um servidor kafka cai, ele para de receber o poll e o servidor percebe entao pega o que esta vivo.


* urls para testes ...
http://localhost:8080/new?email=carlosofpersia1@hotmail.com&amount=1
http://localhost:8080/new?email=carlosofpersia1@hotmail.com&amount=5001 -> acima de 5000 da fraud
http://localhost:8080/admin/generate-reports




**************************************************
-------------------------------------------------
04.2 - Retries e assincronicidade

	common-kafka
		KafkaDispatcher
			send -> sendAndWait (sincrona)
			sendAssync -> assincrona

	* in flight request per session 
		O numero maximo de tentativas paralelas e cinco, mas pode ser configurado.
		* retries 
			maximo de vezes que o sistema vai tentar dar retries de enviar mensagens;
			a ordem dos commits podem ser diferentes;
			valor maior que zero se falhar tenta de novo.
		Sao varias configuracoes para o servidor.
		https://docs.confluent.io/current/installation/configuration/producer-configs.html

**************************************************
-------------------------------------------------
04.3 - Enviando mensagem de deadletter

como verificar os retries
onde estudar configurações importantes do produtor
como implementar um dead letter simples


	* quando eu for consumir a mensagem e da erro.

	em BatchSendMessageService.parse(ConsumerRecord<String... forcar um erro.
		if(true) throw new RuntimeException("deu um erro que eu forcei!");

	esse erro foi gerado, mas a fila ja andou. cade minhas mensagens?


	Para registrar as informacoes quebradas:

		KafkaService.run() { ...
			registrar o erro no catch.
			enviar uma mensagem :P
			
			deadLetter.send("ECOMMERCE_DEADLETTER", ...














--------------------------------------------

Apache Kafka
O Apache Kafka é uma plataforma de streaming distribuída. Através dele é possível processar uma grande quantidade de dados e entregá-los em tempo real aos seus consumidores.

Usado no LinkedIn, Netflix, Twitter e várias outras empresas o Kafka se tornou a ferramenta principal para criar pipeline de dados e enviar, processar e consumir mensagens de forma distribuída, algo muito comum em aplicações baseadas em Microsserviços.

Esta formação foi criada em parceria com o Nubank.


1. Streams, Cluster e Microsserviços
Mergulhe de cabeça no mundo de comunicação assíncrona! Entenda as vantagens do Kafka como broker de mensagens e aprenda como usar Producers, Processors e Consumers. Saiba como se conectar aos serviços externos e aumente a disponibilidade através de um cluster. Veja na pratica como paralelizar e escalar a execução construindo uma solução baseada na arquitetura de microsserviços!


2. Dead Letter Queue e Garantias de entrega
Agora que você já tem bom conhecimento sobre Kafka e a arquitetura de microserviços avance no uso dela. Replica os seus dados e saiba como lidar com erros no processamento. Aprenda como definir um dead letter queue e tentativas de reenvio. Crie múltiplos tópicos para o mesmo serviço e saiba como obter garantias sobre o envio e entrega de mensagens.


--------------------------------------------

* Definição de Pipeline da Dados
Um pipeline de dados é uma série de etapas de processamento de dados. Se os dados não estiverem carregados na plataforma de dados, eles serão ingeridos no início do pipeline. Depois, há uma série de etapas nas quais cada uma fornece uma saída que é a entrada para a próxima etapa. Isso continua até que o pipeline esteja completo. Em alguns casos, etapas independentes podem ser executadas em paralelo.

Os pipelines de dados consistem em três elementos principais: uma fonte, uma ou mais etapas de processamento e um destino. Em alguns pipelines de dados, o destino pode ser chamado de coletor. Os pipelines de dados permitem o fluxo de dados de um aplicativo para um Data Warehouse, de um Data Lake para um banco de dados analítico ou para um sistema de processamento de pagamentos, por exemplo. Os pipelines de dados também podem ter a mesma fonte e coletor, de modo que o pipeline seja apenas para modificar o conjunto de dados. Sempre que os dados são processados ​​entre o ponto A e o ponto B (ou pontos B, C e D), há um pipeline de dados entre esses pontos.

